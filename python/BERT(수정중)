# -*- coding: utf-8 -*-
"""
Created on Wed Dec 23 00:38:33 2020

@author: anhis
"""
# -*- coding: utf-8 -*-
"""
Created on Tue Dec 22 14:47:56 2020

@author: anhis
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install konlpy
!pip install transformers


import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
import seaborn as sn
import re
import json
from konlpy.tag import Okt
import tensorflow as tf
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
from tensorflow.python.keras.preprocessing.text import Tokenizer
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras import layers
from tqdm import tqdm
from transformers import *

Train = pd.read_csv('/content/drive/My Drive/news/news_train.csv')
Test = pd.read_csv('/content/drive/My Drive/news/news_test.csv')

DATA_OUT_PATH = '/content/drive/My Drive/news/'

tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

def bert_tokenizer(sent, MAX_LEN):
    
    encoded_dict = tokenizer.encode_plus(
        text = sent,
        add_special_tokens = True,
        max_length = MAX_LEN,
        padding='max_length',
        return_attention_mask = True,
        truncation=True
        )
    
    input_id = encoded_dict['input_ids']
    attention_mask = encoded_dict['attention_mask']
    
    token_type_id = encoded_dict['token_type_ids']
    
    return input_id, attention_mask, token_type_id

print(tokenizer.all_special_tokens, "\n", tokenizer.all_special_ids)

#Train data preprocessing
input_ids = []
attention_masks = []
token_type_ids = []
train_data_labels = []

def clean_text(sent):
    sent_clean = re.sub("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]", "", sent)
    return sent_clean

for train_sent, train_label in zip(Train['content'], Train['info']):
    try:
        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(train_sent), 42)
        input_ids.append(input_id)
        attention_masks.append(attention_mask)
        token_type_ids.append(token_type_id)
        train_data_labels.append(train_label)
        
    except Exception as e:
        
        print(e)
        print(train_sent)
        pass
    
train_news_input_ids = np.array(input_ids, dtype=int)
train_news_attention_masks = np.array(attention_masks, dtype=int)
train_news_type_ids = np.array(token_type_ids, dtype=int)
train_news_inputs = (train_news_input_ids, train_news_attention_masks, train_news_type_ids)

train_data_labels = np.asarray(train_data_labels, dtype=np.int32)

#Test data preprocessing
input_ids1 = []
attention_masks1 = []
token_type_ids1 = []
train_data_labels1 = []

for test_sent in Test['content']:
    try:
        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(train_sent), 42)
        input_ids1.append(input_id)
        attention_masks1.append(attention_mask)
        token_type_ids1.append(token_type_id)
      
    except Exception as e:
        
        print(e)
        print(train_sent)
        pass
    
test_news_input_ids = np.array(input_ids1, dtype=int)
test_news_attention_masks = np.array(attention_masks1, dtype=int)
test_news_type_ids = np.array(token_type_ids1, dtype=int)
test_news_inputs = (train_news_input_ids1, train_news_attention_masks1, train_news_type_ids1)


print("# sents:{}, #labels: {}".format(len(train_news_input_ids), len(train_data_labels)))

input_id = train_news_input_ids[1]
attention_mask = train_news_attention_masks[1]
token_type_id = train_news_type_ids[1] 

print(input_id)
print(attention_mask)
print(token_type_id)
print(tokenizer.decode(input_id))

class TFBertClassifier(tf.keras.Model):
    def __init__(self, model_name, dir_path, num_class):
        super(TFBertClassifier, self).__init__()
        
        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)
        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)
        self.classifier = tf.keras.layers.Dense(num_class,
                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range),
                                                name="classifier")
        
    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):
        
        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)
        pooled_output = outputs[1]
        pooled_output = self.dropout(pooled_output, training = training)
        logits = self.classifier(pooled_output)
        
        return logits

cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased',
                             dir_path = 'bert_ckpt',
                             num_class=2)

optimizer = tf.keras.optimizers.Adam()
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')

cls_model.compile(optimizer=optimizer,
                  loss=loss,
                  metrics=[metric])

model = cls_model

earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)

model_name = 'bert'
checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weight.h5')
checkpoint_dir = os.path.dirname(checkpoint_path)

if os.path.exists(checkpoint_dir):
    print("{} -- Folder already exists \n".format(checkpoint_dir))
else:
    os.makedirs(checkpoint_dir, exist_ok=True)
    print("{} -- Folder create complete \n".format(checkpoint_dir))
    
cp_callback = ModelCheckpoint(
    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)


history = cls_model.fit(train_news_inputs,
                    train_data_labels,
                    epochs = 2,
                    batch_size=512,
                    validation_split=0.1,
                    callbacks=[earlystop_callback, cp_callback])    

predictions = cls_model.predict(test_inputs)
predictions_label = np.where(predictions >= 0.5, 1, 0)
predictions_label = pd.DataFrame(predictions_label)
predictions_label.to_csv('./CNN_Predictions.csv',header=True)     
