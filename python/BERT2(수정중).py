# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13iBNKSi6Qd06A_2LFTKxbEt-q7GacZHw
"""

import pandas as pd
import numpy as np

from google.colab import drive
drive.mount('/content/drive')
Train = pd.read_csv('/content/drive/MyDrive/datasets/news_train.csv')
Test = pd.read_csv('/content/drive/MyDrive/datasets/news_test.csv')

# train valid 분리하기
train = Train[:85000]
valid = Train[85000:]

train = train.dropna(axis=0)
train = train.reset_index()
valid = valid.dropna(axis=0)
valid = valid.reset_index()
train_text = train['content']
train_label = train['info']
valid_text = valid['content']
valid_label = valid['info']

# BERT에 넣어야 할 데이터는 다음과 같다.
# 1) Input_ids (Tokenizer된 데이터)
# 2) Token_type_ids (문장 유형을 구분한다.)
# 3) Attention_mask_ids (masking을 할 수 있게 해준다)

pip install transformers

import transformers
import torch
from transformers import BertTokenizer

tokenizer= BertTokenizer.from_pretrained("bert-base-uncased", do_lower_case=True)

def text_processing(text, Max_len):
  input_ids = []
  mask_ids = []
  token_type_ids = []
  for line in text:
    # [cls], [sep] 추가 Max_len 길이 맞추기  
    encoded_line = tokenizer.encode(line, add_special_tokens = True, max_length=Max_len, truncation=True) 
    input = encoded_line + [0]*(Max_len-len(encoded_line))
    token_type =  [0]*len(input)
    mask = [1]*len(encoded_line)+[0]*(Max_len-len(encoded_line))    
    input_ids.append(torch.tensor(input))
    token_type_ids.append(torch.tensor(token_type))
    mask_ids.append(torch.tensor(mask))

  return input_ids, token_type_ids, mask_ids
  
train_input_ids, train_token_type_ids, train_mask_ids =text_processing(train_text, 42)
valid_input_ids, valid_token_type_ids, valid_mask_ids =text_processing(valid_text, 42)

# Dataset & Dataloader
from torch.utils.data import Dataset
from torch.utils.data import DataLoader

class BertCls_dataset(Dataset):
  def __init__(self, input_ids, token_type_ids, mask_ids, label):
    self.input = input_ids
    self.token_type = token_type_ids
    self.attn_mask = mask_ids
    self.label = label
  def __len__(self):
    return len(self.input)  

  def __getitem__(self, idx):
    x = self.input[idx]
    y = self.token_type[idx]
    z = self.attn_mask[idx]
    label = self.label[idx]
    label = [label]

    return x, y, z ,torch.tensor(label)

train_dataset=BertCls_dataset(train_input_ids, train_token_type_ids, train_mask_ids, train_label)
valid_dataset=BertCls_dataset(valid_input_ids, valid_token_type_ids, valid_mask_ids, valid_label)

train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
valid_dataloader = DataLoader(valid_dataset, batch_size=16, shuffle=True)

print(train_input_ids)

# Model
import torch
import torch.nn as nn
from transformers import BertModel

bertcls = BertModel.from_pretrained("bert-base-uncased")

class BertCls(nn.Module):
  def __init__(self):
    super().__init__()
    self.bert_cls = bertcls
    self.dropout = nn.Dropout(0.3)
    self.relu = nn.ReLU()
    self.out = nn.Linear(768*2, num_category)

  def forward(self, input_ids, token_type_ids, mask_ids):
    output,_ = self.bert_cls(input_ids, attention_mask=mask_ids, token_type_ids=token_type_ids) #output은 hidden_state
    apool = torch.mean(output, 1)
    mpool, _ = torch.max(output, 1)
    x = torch.cat((apool, mpool), 1) #[cls]를 사용하는 방식 대신, mean, max를 한 후 concat한 output을 사용하였다.
    x = self.relu(x)
    x = self.dropout(x)
    x = self.out(x)

    return x

from tqdm import tqdm
# Train & eval
def train_one_epoch(data_loader, model, optimizer, device, loss_fn):
  
  model.train()
  tk0 = tqdm(data_loader, total=len(data_loader))
  total_loss = 0.0
  
  for bi, d in enumerate(tk0):
      input_ids, token_type_ids, attn_mask_ids, label = d
      input_ids = input_ids.to(device, dtype=torch.long)
      token_type_ids = token_type_ids.to(device, dtype=torch.long)
      attn_mask_ids = attn_mask_ids.to(device, dtype=torch.long)
      label = label.to(device, dtype=torch.long)

      #model.zero_grad()
      output = model(input_ids, token_type_ids, attn_mask_ids)
      loss = loss_fn(output, label.view(-1))
      total_loss += loss.item()
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()

  avg_train_loss = total_loss / len(data_loader) 
  print(" Average training loss: {0:.2f}".format(avg_train_loss))  

def eval_one_epoch(data_loader, model,  device, loss_fn):

  model.eval()
  tk0 = tqdm(data_loader, total=len(data_loader))
  fin_targets = []
  fin_outputs = []
  
  with torch.no_grad():

    for bi, d in enumerate(tk0):
      input_ids, token_type_ids, attn_mask_ids, label = d
      input_ids = input_ids.to(device, dtype=torch.long)
      token_type_ids = token_type_ids.to(device, dtype=torch.long)
      attn_mask_ids = attn_mask_ids.to(device, dtype=torch.long)
      label = label.to(device, dtype=torch.long)

      output = model(input_ids, token_type_ids, attn_mask_ids)
      loss = loss_fn(output, label.view(-1))

      output = output.detach().cpu().numpy()
      label = label.detach().cpu().numpy()
      pred = np.argmax(output, axis=1).flatten()

      fin_targets.extend(label.tolist())
      fin_outputs.extend(pred.tolist()) 

    
  return fin_outputs, fin_targets
  
def fit(train_dataloader, valid_dataloader, EPOCHS=3):
  Bert_cls=BertCls() #model
  Bert_cls=Bert_cls.to(device)
  loss_fn = nn.CrossEntropyLoss() #loss
  optimizer = torch.optim.AdamW(Bert_cls.parameters(),lr=lr) #optimizer

  for i in range(EPOCHS):
    print(f"EPOCHS:{i+1}")
    print('TRAIN')
    train_one_epoch(train_dataloader, Bert_cls, optimizer, device, loss_fn)
    print('EVAL')
    outputs, targets = eval_one_epoch(valid_dataloader, Bert_cls,  device, loss_fn)
    targets = np.array(targets)
    auc = accuracy_score(targets, outputs)
    print(f"auc;{auc}")  
    
    
Max_len = 42
Batch_size = 16
num_category = 3
lr = 3e-5
device='cuda'
EPOCHS=3

fit(train_dataloader, valid_dataloader)